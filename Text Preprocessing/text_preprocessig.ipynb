{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb7c9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     text  num\n",
      "0            Movies is Very easy<br></br>   10\n",
      "1  <html>!@ movies is very bad. very bad    20\n",
      "2                        Movies are good    30\n",
      "3      <span>movies are excellent?</span>   40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data={\n",
    "    \"text\":[\"Movies is Very easy<br></br>\",\"<html>!@ movies is very bad. very bad \",\"Movies are good \",\"<span>movies are excellent?</span>\"],\n",
    "    \"num\":[10,20,30,40],\n",
    "    \n",
    "}\n",
    "df=pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17962db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     text  num\n",
      "0            movies is very easy<br></br>   10\n",
      "1  <html>!@ movies is very bad. very bad    20\n",
      "2                        movies are good    30\n",
      "3      <span>movies are excellent?</span>   40\n"
     ]
    }
   ],
   "source": [
    "#lowercasing\n",
    "df['text']=df['text'].str.lower()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b7e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove html tags\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85f8a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               text  num\n",
      "0               movies is very easy   10\n",
      "1  !@ movies is very bad. very bad    20\n",
      "2                  movies are good    30\n",
      "3             movies are excellent?   40\n"
     ]
    }
   ],
   "source": [
    "df['text']=df['text'].apply(remove_html_tags)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ca4968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URLS\n",
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f828c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am learning this from  from \n"
     ]
    }
   ],
   "source": [
    "text1=\"i am learning this from https://www.youtube.com/watch?v=6C0sLtw5ctc from www.google.com\"\n",
    "text1=remove_url(text1)\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6ccd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove punctuation\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514bb5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=string.punctuation\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',punc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "631a39b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " movies is very bad very bad\n"
     ]
    }
   ],
   "source": [
    "t=\"!@ movies is very bad. very bad\"\n",
    "t=remove_punc(t)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f67c3391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            text  num\n",
      "0            movies is very easy   10\n",
      "1   movies is very bad very bad    20\n",
      "2               movies are good    30\n",
      "3           movies are excellent   40\n"
     ]
    }
   ],
   "source": [
    "df['text']=df['text'].apply(remove_punc)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6043d314",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Spelling Correction\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[32m      4\u001b[39m incorrect_text=\u001b[33m\"\u001b[39m\u001b[33minvaild text aree yuo bacer exceelent\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m textBlb=TextBlob(incorrect_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vansh Parmar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Blobber, Sentence, TextBlob, Word, WordList\n\u001b[32m      3\u001b[39m __all__ = [\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTextBlob\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWord\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWordList\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vansh Parmar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextblob\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     BaseNPExtractor,\n\u001b[32m     31\u001b[39m     BaseParser,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     BaseTokenizer,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextblob\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cached_property, requires_nltk_corpus\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vansh Parmar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\__init__.py:180\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Check if tkinter exists without importing it to avoid crashes after\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# forks on macOS. Only nltk.app, nltk.draw, and demo modules should\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# have top-level tkinter imports. See #2949 for more details.\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m importlib.util.find_spec(\u001b[33m\"\u001b[39m\u001b[33mtkinter\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vansh Parmar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\downloader.py:2528\u001b[39m\n\u001b[32m   2518\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2521\u001b[39m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[32m   2522\u001b[39m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[32m   2523\u001b[39m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2526\u001b[39m \n\u001b[32m   2527\u001b[39m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2528\u001b[39m _downloader = \u001b[43mDownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2529\u001b[39m download = _downloader.download\n\u001b[32m   2532\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_shell\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vansh Parmar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\downloader.py:503\u001b[39m, in \u001b[36mDownloader.__init__\u001b[39m\u001b[34m(self, server_index_url, download_dir)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28mself\u001b[39m._download_dir = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_download_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vansh Parmar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\downloader.py:1056\u001b[39m, in \u001b[36mDownloader.default_download_dir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m.path:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk.internals.is_writable(nltkdir):\n\u001b[32m   1058\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "#Spelling Correction\n",
    "from textblob import TextBlob\n",
    "\n",
    "incorrect_text=\"invaild text aree yuo bacer exceelent\"\n",
    "textBlb=TextBlob(incorrect_text)\n",
    "\n",
    "textBlb.correct().string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44dca59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a278a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc6bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                text  num\n",
      "0      movies   easy   10\n",
      "1  movies   bad  bad   20\n",
      "2       movies  good   30\n",
      "3  movies  excellent   40\n"
     ]
    }
   ],
   "source": [
    "df['text']=df['text'].apply(remove_stopwords)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09acd205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'done', 'ph.D.', 'in', 'A.I', '.']\n",
      "['we', \"'re\", 'here', 'to', 'help', '!', 'mail', 'me', 'at', 'abc', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "txt1=\"I done ph.D. in A.I.\"\n",
    "txt2=\"we're here to help! mail me at abc@gmail.com\"\n",
    "txt3='A 5km ride cost $10.50'\n",
    "\n",
    "print(word_tokenize(txt1))\n",
    "print(word_tokenize(txt2))\n",
    "print(word_tokenize(txt3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2922d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I done ph.D. in A.I.',\n",
       " \"we're here to help!\",\n",
       " 'mail me at abc@gmail.com.',\n",
       " 'A 5km ride cost $10.50']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=\"I done ph.D. in A.I. we're here to help! mail me at abc@gmail.com. A 5km ride cost $10.50\"\n",
    "sent_tokenize(txt)\n",
    "#we can also use spacy\n",
    "#import spacy\n",
    "#nlp=spacy.load('en_core_web_sm')\n",
    "#convert sentence in doc\n",
    "#doc=nlp(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "931293cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join(ps.stem(word) for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca5675e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=\"walk walking walked walks\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd5236d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl i will goe for movi watch through walk'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1=\"probably i will goes for movie watching through walking\"\n",
    "stem_words(sample1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
